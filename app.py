import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    pipeline  # Import pipeline cho Guardrail
)
from peft import PeftModel
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict 
import os 

# --- 1. T·∫¢I MODEL "B·∫¢O V·ªÜ" (Guardrail) ---
print("--- ƒêANG T·∫¢I MODEL B·∫¢O V·ªÜ (Guardrail) ---")
GUARDRAIL_PATH = r"D:\Work\AI\guardrail_model\checkpoint-950"

if not os.path.exists(GUARDRAIL_PATH):
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{GUARDRAIL_PATH}'.")
    print("Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n v√† hu·∫•n luy·ªán model 'B·∫£o V·ªá' tr∆∞·ªõc.")
    exit()

guardrail_pipeline = pipeline(
    "text-classification", 
    model=GUARDRAIL_PATH, 
    tokenizer=GUARDRAIL_PATH, 
    device=-1 # √âp ch·∫°y tr√™n CPU, kh√¥ng t·ªën VRAM
)
print("--- MODEL B·∫¢O V·ªÜ ƒê√É S·∫¥N S√ÄNG (TR√äN CPU) ---")


# --- 2. T·∫¢I MODEL "REFRAME BOT" (LLM) ---
print("--- B·∫ÆT ƒê·∫¶U T·∫¢I MODEL LLM (C√ì TH·ªÇ M·∫§T V√ÄI PH√öT) ---")
base_model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct" 
adapter_path = r"D:\Work\AI\results_reframebot_DPO\checkpoint-90" 

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.pad_token = tokenizer.eos_token
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, adapter_path)
model = model.merge_and_unload() 
model.eval()
print("--- MODEL LLM ƒê√É S·∫¥N S√ÄNG (TR√äN GPU) ---")


# --- 3. KH·ªûI T·∫†O API SERVER V√Ä C√ÅC C√ÇU TR·∫¢ L·ªúI C·ª®NG ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"], 
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    history: List[Dict[str, str]] 

# Ch·ªâ gi·ªØ ph·∫ßn text c·ªßa hotline, v√¨ ph·∫ßn th·∫•u c·∫£m s·∫Ω do LLM gen
VIETNAMESE_HOTLINES = (
    "Please reach out to these resources in Vietnam:\n\n"
    "**1. National Protection Hotline:** 1900 1267\n"
    "**2. 'Ngay Mai' Hotline (Depression & Suicide Prevention):** 096 306 1414\n"
    "**3. Emergency Services:** 113 or 115\n"
    "**4. Depression Emergency Hotline:** 1900 1267\n\n"
    "Please reach out for help immediately. There are people who care about you."
)

CRISIS_CONFIDENCE_THRESHOLD = 0.90


# --- 4. C√ÅC H√ÄM T·∫†O RESPONSE (LLM) ---

# H√ÄM 1: D√πng cho Task 1 (CBT) v√† Task 3 (OOS)
def get_response_llm(message_history: List[Dict[str, str]], task_label: str):
    
    # --- System Prompt ƒê·ªông (Dynamic System Prompt) ---
    base_system_prompt = """
You are ReframeBot, a specialized AI assistant. Your primary goal is to help university students with academic stress using CBT Socratic questioning.
You MUST follow these 3 rules at all times:
1.  **TASK 1 (CBT):** If the user is discussing **academic stress**... you MUST respond with (1) Empathy, then (2) Socratic Questions.
2.  **TASK 2 (CRISIS):** If the user expresses **ANY** thought of suicide... you MUST **STOP**! and redirect to a hotline.
3.  **TASK 3 (OUT-OF-SCOPE):** If the user discusses **non-academic** topics... you MUST **STOP**! (1) Validate their feeling, then (2) Gently state your limitation and pivot back to academics.
Do not give direct advice. Do not diagnose.
"""
    
    # Can thi·ªáp (inject) prompt n·∫øu "B·∫£o V·ªá" ph√°t hi·ªán Task 3
    if task_label == "TASK_3":
        critical_instruction = (
            "\n\n**CRITICAL INSTRUCTION:** The user's last message was identified as **Out-of-Scope (TASK 3)**. "
            "You MUST follow TASK 3 rules. **DO NOT** ask follow-up questions about their non-academic topic. "
            "Validate the feeling, state your limitation, and pivot back to academics NOW."
        )
        system_prompt = base_system_prompt + critical_instruction
    else:
        system_prompt = base_system_prompt

    messages = [
        {"role": "system", "content": system_prompt}
    ]
    messages.extend(message_history)
    
    prompt_string = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False
    )
    
    inputs = tokenizer(
        prompt_string, 
        return_tensors="pt",
        padding=False
    ).to(model.device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=256, 
            eos_token_id=terminators, 
            do_sample=True,
            temperature=0.6, 
            top_p=0.9,
        )
    
    response_ids = outputs[0][inputs.input_ids.shape[-1]:]
    response = tokenizer.decode(response_ids, skip_special_tokens=True)

    # An to√†n k√©p: V·∫´n l·ªçc hotline M·ªπ (n·∫øu l·ª°)
    CRISIS_TRIGGERS = [
        "1-800-273", "741741", "hotline", "lifeline"
    ]
    response_lower = response.lower()
    if any(trigger in response_lower for trigger in CRISIS_TRIGGERS):
        response = "I am deeply concerned for your safety.\n" + VIETNAMESE_HOTLINES

    return response

# H√ÄM 2 (M·ªöI): Ch·ªâ d√πng cho Task 2 (Kh·ªßng ho·∫£ng)
def get_crisis_empathy_llm(message_history: List[Dict[str, str]]):
    
    # Prompt "√©p" model ch·ªâ l√†m 1 vi·ªác: Th·∫•u c·∫£m 1 c√¢u
    system_prompt = (
        "You are an empathetic listener. A user is in severe crisis. "
        "Your ONLY job is to respond with **one or two sentences** that validates their pain and shows deep concern. "
        "DO NOT ask questions. DO NOT give advice. DO NOT use the word 'hotline' or 'resources'."
    )
    
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    # Ch·ªâ l·∫•y 2 tin nh·∫Øn cu·ªëi c·ªßa user ƒë·ªÉ l√†m "m·ªìi"
    messages.extend(message_history[-2:]) 
    
    prompt_string = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False
    )
    inputs = tokenizer(prompt_string, return_tensors="pt", padding=False).to(model.device)
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=64, # Ch·ªâ gen 1 c√¢u ng·∫Øn
            eos_token_id=terminators, 
            do_sample=True,
            temperature=0.5,
            top_p=0.9,
        )
    
    response_ids = outputs[0][inputs.input_ids.shape[-1]:]
    empathy_response = tokenizer.decode(response_ids, skip_special_tokens=True)
    
    return empathy_response


# --- 5. ƒê·ªäNH NGHƒ®A ENDPOINT (LOGIC HYBRID M·ªöI) ---
@app.get("/")
def read_root():
    return {"message": "ReframeBot API (v·ªõi Hybrid Guardrail + Empathy) ƒëang ch·∫°y!"}

@app.post("/chat")
def chat_endpoint(request: ChatRequest):
    
    user_history = request.history
    if not user_history:
        return {"response": "Hello! Please start the conversation."}
    
    last_user_prompt = user_history[-1]['content']
    print(f"\n[Request] Prompt: '{last_user_prompt}'")
    
    # 2. H·ªéI "B·∫¢O V·ªÜ" TR∆Ø·ªöC (TR√äN CPU)
    guardrail_result = guardrail_pipeline(last_user_prompt)[0]
    label = guardrail_result['label'] 
    score = guardrail_result['score']
    
    print(f"[Guardrail Check] Label: {label} (Score: {score:.4f})")

    # 3. LOGIC HYBRID
    
    if label == "TASK_2" and score >= CRISIS_CONFIDENCE_THRESHOLD:
        # Kh·ªßng ho·∫£ng (T·ª± tin cao): Gen th·∫•u c·∫£m + N·ªëi hotline
        print("üî¥ Guardrail: TASK_2 (High Score) detected. Calling EMPATHY LLM...")
        
        # B∆∞·ªõc 1: G·ªçi LLM ch·ªâ ƒë·ªÉ gen 1 c√¢u th·∫•u c·∫£m
        empathy_part = get_crisis_empathy_llm(user_history)
        
        # B∆∞·ªõc 2: N·ªëi (append) hotline c·ª©ng v√†o
        full_response = empathy_part + "\n\n" + VIETNAMESE_HOTLINES
        
        print("‚úÖ LLM Empathy Response Sent (with hotlines).")
        return {"response": full_response}
    
    else: 
        # (N·∫øu l√† TASK_1, ho·∫∑c TASK_3, ho·∫∑c TASK_2 (Low Score))
        
        # X√°c ƒë·ªãnh "nh√£n hi·ªáu qu·∫£"
        effective_label = label
        if label == "TASK_2" and score < CRISIS_CONFIDENCE_THRESHOLD:
            # S·ª≠a l·ªói "I want to make money"
            print(f"üü° Guardrail: TASK_2 (Low Score) detected. Overriding to TASK_3.")
            effective_label = "TASK_3"
            
        print(f"üü¢ Guardrail: Effective Label={effective_label}. Calling FULL LLM...")
        bot_response = get_response_llm(user_history, effective_label)
        print("‚úÖ Full LLM Response Sent.")
        return {"response": bot_response}

# --- 6. CH·∫†Y SERVER ---
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)