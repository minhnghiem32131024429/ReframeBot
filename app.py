import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    pipeline  # Import pipeline cho Guardrail
)
from peft import PeftModel
import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict 
import os
import chromadb
from sentence_transformers import SentenceTransformer 

# --- 1. T·∫¢I MODEL "B·∫¢O V·ªÜ" (Guardrail) ---
print("--- ƒêANG T·∫¢I MODEL B·∫¢O V·ªÜ (Guardrail) ---")
GUARDRAIL_PATH = r"D:\Work\AI\guardrail_model\checkpoint-950"

if not os.path.exists(GUARDRAIL_PATH):
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{GUARDRAIL_PATH}'.")
    print("Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n v√† hu·∫•n luy·ªán model 'B·∫£o V·ªá' tr∆∞·ªõc.")
    exit()

guardrail_pipeline = pipeline(
    "text-classification", 
    model=GUARDRAIL_PATH, 
    tokenizer=GUARDRAIL_PATH, 
    device=-1 # √âp ch·∫°y tr√™n CPU, kh√¥ng t·ªën VRAM
)
print("--- MODEL B·∫¢O V·ªÜ ƒê√É S·∫¥N S√ÄNG (TR√äN CPU) ---")


# --- 1.5 T·∫¢I RAG SYSTEM ---
print("--- ƒêANG T·∫¢I RAG DATABASE ---")
RAG_DB_PATH = "./rag_db"
if not os.path.exists(RAG_DB_PATH):
    print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c RAG '{RAG_DB_PATH}'.")
    print("B·∫°n c·∫ßn ch·∫°y script 'build_rag_db.py' tr∆∞·ªõc ƒë·ªÉ t·∫°o database.")
    # (V·∫´n ch·∫°y ti·∫øp, nh∆∞ng RAG s·∫Ω kh√¥ng ho·∫°t ƒë·ªông)
    rag_collection = None
else:
    rag_embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Model nh√∫ng (ch·∫°y CPU)
    rag_client = chromadb.PersistentClient(path=RAG_DB_PATH)
    # ƒê·∫£m b·∫£o t√™n collection kh·ªõp v·ªõi script t·∫°o DB c·ªßa b·∫°n (vd: "cbt_knowledge")
    rag_collection = rag_client.get_collection(name="cbt_knowledge") 
print("--- RAG DATABASE ƒê√É S·∫¥N S√ÄNG ---")


# --- 2. T·∫¢I MODEL "REFRAME BOT" (LLM) ---
print("--- B·∫ÆT ƒê·∫¶U T·∫¢I MODEL LLM (C√ì TH·ªÇ M·∫§T V√ÄI PH√öT) ---")
base_model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct" 
adapter_path = r"D:\Work\AI\results_reframebot_DPO\checkpoint-90" 

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.pad_token = tokenizer.eos_token
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, adapter_path)
model = model.merge_and_unload() 
model.eval()
print("--- MODEL LLM ƒê√É S·∫¥N S√ÄNG (TR√äN GPU) ---")


# --- 3. KH·ªûI T·∫†O API SERVER V√Ä C√ÅC C√ÇU TR·∫¢ L·ªúI C·ª®NG ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"], 
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    history: List[Dict[str, str]] 

# Ch·ªâ gi·ªØ ph·∫ßn text c·ªßa hotline, v√¨ ph·∫ßn th·∫•u c·∫£m s·∫Ω do LLM gen
VIETNAMESE_HOTLINES = (
    "Please reach out to these resources in Vietnam:\n\n"
    "**1. National Protection Hotline:** 1900 1267\n"
    "**2. 'Ngay Mai' Hotline (Depression & Suicide Prevention):** 096 306 1414\n"
    "**3. Emergency Services:** 113 or 115\n"
    "**4. Depression Emergency Hotline:** 1900 1267\n\n"
    "Please reach out for help immediately. There are people who care about you."
)

CRISIS_CONFIDENCE_THRESHOLD = 0.90
ACADEMIC_KEYWORDS = [
    "pomodoro", "cbt", "cognitive behavioral therapy", 
    "smart goals", "mind map", "active recall", 
    "spaced repetition", "feynman", "imposter syndrome",
    "burnout", "distortion", "catastrophizing"
]

# --- 3.5 H√ÄM TRUY XU·∫§T KI·∫æN TH·ª®C T·ª™ RAG ---
def retrieve_knowledge(user_query: str, top_k: int = 3) -> str:
    """
    T√¨m ki·∫øm ki·∫øn th·ª©c li√™n quan t·ª´ RAG database
    """
    if rag_collection is None:
        return "" # Tr·∫£ v·ªÅ r·ªóng n·∫øu RAG ch∆∞a load ƒë∆∞·ª£c

    try:
        # Nh√∫ng c√¢u h·ªèi c·ªßa user
        query_embedding = rag_embedder.encode([user_query]).tolist()
        
        # T√¨m ki·∫øm top_k ƒëo·∫°n vƒÉn b·∫£n t∆∞∆°ng t·ª± nh·∫•t
        results = rag_collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )
        
        # Gh√©p c√°c ƒëo·∫°n ki·∫øn th·ª©c th√†nh 1 chu·ªói
        if results['documents'] and len(results['documents'][0]) > 0:
            # L·ªçc b·ªõt c√°c k·∫øt qu·∫£ kh√¥ng li√™n quan (n·∫øu c·∫ßn) ho·∫∑c l·∫•y h·∫øt
            knowledge = "\n\n".join(results['documents'][0])
            return knowledge
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói RAG: {e}")
        return ""
        
    return ""


# --- 4. C√ÅC H√ÄM T·∫†O RESPONSE (LLM V·ªöI RAG) ---

# H√ÄM 1: D√πng cho Task 1 (CBT) v√† Task 3 (OOS) - C√ì RAG
def get_response_llm(message_history: List[Dict[str, str]], task_label: str):
    
    # L·∫•y c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa user
    last_user_message = message_history[-1]['content'] if message_history else ""
    
    # Truy xu·∫•t ki·∫øn th·ª©c t·ª´ RAG (ch·ªâ cho TASK_1 - CBT)
    rag_context = ""
    if task_label == "TASK_1" and last_user_message:
        print(f"\nüîç [RAG Check] ƒêang t√¨m ki·∫øn th·ª©c cho: '{last_user_message[:30]}...'")
        rag_context = retrieve_knowledge(last_user_message, top_k=2) # L·∫•y 2 ƒëo·∫°n t·ªët nh·∫•t
        if rag_context:
            print(f"‚úÖ [RAG Found] ƒê√£ t√¨m th·∫•y {len(rag_context)} k√Ω t·ª± ki·∫øn th·ª©c")
            # print(f"   -> Content: {rag_context[:50]}...") # (B·ªè comment ƒë·ªÉ debug)
    
    # --- System Prompt ƒê·ªông (Dynamic System Prompt v·ªõi RAG) ---
    base_system_prompt = """
You are ReframeBot, a specialized AI assistant. Your primary goal is to help university students with academic stress using CBT Socratic questioning.
You MUST follow these 3 rules at all times:
1.  **TASK 1 (CBT):** If the user is discussing **academic stress**... you MUST respond with (1) Empathy, then (2) Socratic Questions.
2.  **TASK 2 (CRISIS):** If the user expresses **ANY** thought of suicide... you MUST **STOP**! and redirect to a hotline.
3.  **TASK 3 (OUT-OF-SCOPE):** If the user discusses **non-academic** topics... you MUST **STOP**! (1) Validate their feeling, then (2) Gently state your limitation and pivot back to academics.
Do not give direct advice. Do not diagnose.
"""
    
    # Th√™m RAG context v√†o prompt n·∫øu c√≥
    if rag_context:
        base_system_prompt += f"""

**KNOWLEDGE BASE REFERENCE:**
The following information from the CBT knowledge base may help guide your response:

{rag_context}

Use this information to explain the concept to the student clearly. 
You CAN define terms and explain steps if the user asks "What is...".
However, after explaining, always try to link it back to their feelings or ask if they want to try it.
"""
    
    # Can thi·ªáp (inject) prompt n·∫øu "B·∫£o V·ªá" ph√°t hi·ªán Task 3
    if task_label == "TASK_3":
        critical_instruction = (
            "\n\n**CRITICAL INSTRUCTION:** The user's last message was identified as **Out-of-Scope (TASK 3)**. "
            "You MUST follow TASK 3 rules. **DO NOT** ask follow-up questions about their non-academic topic. "
            "Validate the feeling, state your limitation, and pivot back to academics NOW."
        )
        system_prompt = base_system_prompt + critical_instruction
    else:
        system_prompt = base_system_prompt

    messages = [
        {"role": "system", "content": system_prompt}
    ]
    messages.extend(message_history)
    
    prompt_string = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False
    )
    
    inputs = tokenizer(
        prompt_string, 
        return_tensors="pt",
        padding=False
    ).to(model.device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=512, 
            eos_token_id=terminators, 
            do_sample=True,
            temperature=0.6, 
            top_p=0.9,
        )
    
    response_ids = outputs[0][inputs.input_ids.shape[-1]:]
    response = tokenizer.decode(response_ids, skip_special_tokens=True)

    # An to√†n k√©p: V·∫´n l·ªçc hotline M·ªπ (n·∫øu l·ª°)
    CRISIS_TRIGGERS = [
        "1-800-273", "741741", "hotline", "lifeline"
    ]
    response_lower = response.lower()
    if any(trigger in response_lower for trigger in CRISIS_TRIGGERS):
        response = "I am deeply concerned for your safety.\n" + VIETNAMESE_HOTLINES

    return response

# H√ÄM 2 (M·ªöI): Ch·ªâ d√πng cho Task 2 (Kh·ªßng ho·∫£ng)
def get_crisis_empathy_llm(message_history: List[Dict[str, str]]):
    
    # Prompt "√©p" model ch·ªâ l√†m 1 vi·ªác: Th·∫•u c·∫£m 1 c√¢u
    system_prompt = (
        "You are an empathetic listener. A user is in severe crisis. "
        "Your ONLY job is to respond with **one or two sentences** that validates their pain and shows deep concern. "
        "DO NOT ask questions. DO NOT give advice. DO NOT use the word 'hotline' or 'resources'."
    )
    
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    # Ch·ªâ l·∫•y 2 tin nh·∫Øn cu·ªëi c·ªßa user ƒë·ªÉ l√†m "m·ªìi"
    messages.extend(message_history[-2:]) 
    
    prompt_string = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=False
    )
    inputs = tokenizer(prompt_string, return_tensors="pt", padding=False).to(model.device)
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=64, # Ch·ªâ gen 1 c√¢u ng·∫Øn
            eos_token_id=terminators, 
            do_sample=True,
            temperature=0.5,
            top_p=0.9,
        )
    
    response_ids = outputs[0][inputs.input_ids.shape[-1]:]
    empathy_response = tokenizer.decode(response_ids, skip_special_tokens=True)
    
    return empathy_response


# --- 5. ƒê·ªäNH NGHƒ®A ENDPOINT (LOGIC HYBRID M·ªöI) ---
@app.get("/")
def read_root():
    return {"message": "ReframeBot API (v·ªõi Hybrid Guardrail + Empathy + RAG) ƒëang ch·∫°y!"}

@app.post("/chat")
def chat_endpoint(request: ChatRequest):
    
    user_history = request.history
    if not user_history:
        return {"response": "Hello! Please start the conversation."}
    
    last_user_prompt = user_history[-1]['content']
    print(f"\n[Request] Prompt: '{last_user_prompt}'")
    
    # 2. H·ªéI "B·∫¢O V·ªÜ"
    guardrail_result = guardrail_pipeline(last_user_prompt)[0]
    label = guardrail_result['label'] 
    score = guardrail_result['score']
    print(f"[Guardrail Check] Label: {label} (Score: {score:.4f})")

    # <<< LOGIC M·ªöI: KEYWORD OVERRIDE (S·ª¨A ·ªû ƒê√ÇY) >>>
    
    # Ki·ªÉm tra xem c√≥ t·ª´ kh√≥a h·ªçc thu·∫≠t n√†o kh√¥ng?
    # <<< LOGIC M·ªöI ƒê√É S·ª¨A: KI·ªÇM TRA NG·ªÆ C·∫¢NH (CONTEXT AWARE) >>>
    
    # L·∫•y n·ªôi dung c·ªßa 3 tin nh·∫Øn g·∫ßn nh·∫•t (User v√† Bot) ƒë·ªÉ ki·ªÉm tra ng·ªØ c·∫£nh
    # Vi·ªác n√†y gi√∫p model hi·ªÉu "Yes, please" l√† ƒëang n√≥i v·ªÅ ch·ªß ƒë·ªÅ tr∆∞·ªõc ƒë√≥
    recent_context = ""
    recent_messages = user_history[-3:] # L·∫•y 3 tin cu·ªëi
    for msg in recent_messages:
        recent_context += msg['content'].lower() + " "
        
    # Ki·ªÉm tra t·ª´ kh√≥a trong TO√ÄN B·ªò ng·ªØ c·∫£nh g·∫ßn ƒë√¢y
    has_academic_keyword = any(kw in recent_context for kw in ACADEMIC_KEYWORDS)
    
    if has_academic_keyword:
        print(f"üîµ Keyword Override: Academic term detected. Forcing TASK_1.")
        effective_label = "TASK_1"
        
    elif label == "TASK_2" and score >= CRISIS_CONFIDENCE_THRESHOLD:
        # Kh·ªßng ho·∫£ng th·∫≠t s·ª± -> Ch·∫∑n lu√¥n
        print("üî¥ Guardrail: TASK_2 (High Score) detected. Calling EMPATHY LLM...")
        empathy_part = get_crisis_empathy_llm(user_history)
        full_response = empathy_part + "\n\n" + VIETNAMESE_HOTLINES
        return {"response": full_response}
        
    elif label == "TASK_2" and score < CRISIS_CONFIDENCE_THRESHOLD:
        # Kh·ªßng ho·∫£ng gi·∫£ -> Task 3
        print(f"üü° Guardrail: TASK_2 (Low Score) detected. Overriding to TASK_3.")
        effective_label = "TASK_3"
        
    else:
        # Gi·ªØ nguy√™n nh√£n c·ªßa b·∫£o v·ªá (Task 1 ho·∫∑c Task 3)
        effective_label = label

    # --- G·ªçi LLM ---
    print(f"üü¢ Guardrail: Effective Label={effective_label}. Calling FULL LLM...")
    bot_response = get_response_llm(user_history, effective_label)
    return {"response": bot_response}

# --- 6. CH·∫†Y SERVER ---
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)