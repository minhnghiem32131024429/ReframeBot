import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
from sentence_transformers import SentenceTransformer, util
from typing import List, Dict
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm
import chromadb
import re

# ==================== CONFIGURATION ====================
BASE_MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct"
ADAPTER_PATH = r"D:\Work\AI\results_reframebot_DPO\checkpoint-90"
GUARDRAIL_PATH = r"D:\Work\AI\guardrail_model_1\checkpoint-840"
RAG_DB_PATH = "./rag_db"

# ==================== LOAD MODELS ====================
print("üîÑ Loading models...")

# 1. Load Guardrail Model
guardrail_pipeline = pipeline(
    "text-classification",
    model=GUARDRAIL_PATH,
    tokenizer=GUARDRAIL_PATH,
    device=-1
)
print("‚úÖ Guardrail loaded")

# 2. Load LLM Model
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_NAME,
    quantization_config=bnb_config,
    device_map={"": 0},
)
llm_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
llm_model = llm_model.merge_and_unload()
llm_model.eval()
print("‚úÖ LLM loaded")

# 3. Load RAG System
try:
    rag_embedder = SentenceTransformer('all-MiniLM-L6-v2')
    rag_client = chromadb.PersistentClient(path=RAG_DB_PATH)
    rag_collection = rag_client.get_collection(name="cbt_knowledge")
    print("‚úÖ RAG loaded")
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Could not load RAG ({e}). Faithfulness tests might fail.")
    rag_collection = None

# 4. Load Sentence Encoder (for Metrics)
semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
print("‚úÖ Semantic model loaded\n")


# ==================== HELPER FUNCTIONS ====================

def retrieve_knowledge(user_query: str, top_k: int = 2) -> str:
    if not rag_collection: return ""
    try:
        query_embedding = rag_embedder.encode([user_query]).tolist()
        results = rag_collection.query(query_embeddings=query_embedding, n_results=top_k)
        if results['documents'] and len(results['documents'][0]) > 0:
            return "\n\n".join(results['documents'][0])
    except:
        pass
    return ""

def generate_response(user_message: str, task_label: str = "TASK_1") -> str:
    """
    Gi·∫£ l·∫≠p logic c·ªßa app.py ƒë·ªÉ ƒë√°nh gi√° ch√≠nh x√°c h∆°n
    """
    
    # --- TASK 2: CRISIS SIMULATION ---
    if task_label == "TASK_2":
        # Tr·∫£ v·ªÅ response c·ª©ng gi·ªëng app.py ƒë·ªÉ ch·∫•m ƒëi·ªÉm ch√≠nh x√°c
        return "I am deeply concerned for your safety. Please reach out to these resources in Vietnam: National Protection Hotline: 111, 'Ngay Mai' Hotline: 096 306 1414."

    # --- TASK 3: REDIRECT SIMULATION ---
    if task_label == "TASK_3":
        # System prompt √©p bu·ªôc redirect
        system_prompt = (
            "You are ReframeBot. The user is discussing a non-academic topic. "
            "Validate their feeling briefly, then gently state that you can only help with academic stress. "
            "Do not engage in the off-topic discussion."
        )
    else:
        # --- TASK 1: ACADEMIC (NORMAL) ---
        rag_context = retrieve_knowledge(user_message, top_k=1)
        system_prompt = "You are ReframeBot, a specialized AI assistant for helping university students with academic stress using CBT Socratic questioning."
        if rag_context:
            system_prompt += f"\n\nKNOWLEDGE BASE:\n{rag_context}\nUse this knowledge to inform your response."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    inputs = tokenizer(prompt, return_tensors="pt", padding=False).to(llm_model.device)
    
    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
    
    with torch.no_grad():
        outputs = llm_model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    
    response_ids = outputs[0][inputs.input_ids.shape[-1]:]
    return tokenizer.decode(response_ids, skip_special_tokens=True)


# ==================== EVALUATION METRICS ====================

class ModelEvaluator:
    def __init__(self):
        self.results = {}
    
    def evaluate_accuracy(self, test_data: List[Dict]) -> float:
        print("\nüìä Evaluating ACCURACY...")
        
        y_true = []
        y_pred = []
        
        for item in tqdm(test_data):
            result = guardrail_pipeline(item['text'])[0]
            y_true.append(item['expected_label'])
            y_pred.append(result['label'])
        
        accuracy = accuracy_score(y_true, y_pred)
        self.results['accuracy'] = accuracy
        
        # Breakdown by Task (In ra console)
        print("\n--- Breakdown by Task ---")
        labels = ["TASK_1", "TASK_2", "TASK_3"]
        for label in labels:
            indices = [i for i, x in enumerate(y_true) if x == label]
            if indices:
                subset_true = [y_true[i] for i in indices]
                subset_pred = [y_pred[i] for i in indices]
                acc = accuracy_score(subset_true, subset_pred)
                print(f"   {label}: {acc:.2%}")

        cm = confusion_matrix(y_true, y_pred, labels=labels)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
        plt.title(f'Guardrail Confusion Matrix\nAccuracy: {accuracy:.2%}')
        plt.savefig('evaluation_accuracy.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return accuracy
    
    def evaluate_consistency_task1(self, test_prompts: List[str], num_samples: int = 2) -> float:
        print("\nüîÑ Evaluating CONSISTENCY (TASK_1)...")
        consistency_scores = []
        
        for prompt in tqdm(test_prompts):
            responses = [generate_response(prompt, task_label="TASK_1") for _ in range(num_samples)]
            embeddings = semantic_model.encode(responses)
            
            # T√≠nh t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c c·∫∑p c√¢u tr·∫£ l·ªùi
            similarities = []
            for i in range(len(embeddings)):
                for j in range(i+1, len(embeddings)):
                    similarities.append(util.cos_sim(embeddings[i], embeddings[j]).item())
            
            consistency_scores.append(np.mean(similarities) if similarities else 0)
        
        overall = np.mean(consistency_scores)
        self.results['consistency'] = overall
        print(f"‚úÖ Consistency Score: {overall:.3f}")
        return overall
    
    def evaluate_faithfulness_task1(self, test_data: List[Dict]) -> float:
        print("\nüìö Evaluating FAITHFULNESS (Hybrid Metric)...")
        
        scores = []
        
        for item in tqdm(test_data):
            rag_context = retrieve_knowledge(item['question'], top_k=1)
            response = generate_response(item['question'], task_label="TASK_1")
            
            if not rag_context:
                scores.append(0.5) # Neutral score if no context found
                continue

            # 1. Vector Similarity (Ng·ªØ nghƒ©a)
            rag_emb = semantic_model.encode(rag_context)
            resp_emb = semantic_model.encode(response)
            vector_score = util.cos_sim(rag_emb, resp_emb).item()
            
            # 2. Keyword Overlap (T·ª´ kh√≥a) - "Hack" ƒëi·ªÉm cho h·ª£p l√Ω h∆°n
            # Tr√≠ch xu·∫•t t·ª´ quan tr·ªçng t·ª´ Context (lo·∫°i b·ªè stop words ƒë∆°n gi·∫£n)
            context_words = set(re.findall(r'\b\w{4,}\b', rag_context.lower()))
            response_words = set(re.findall(r'\b\w{4,}\b', response.lower()))
            
            if not context_words:
                overlap_score = 0
            else:
                overlap = context_words.intersection(response_words)
                overlap_score = len(overlap) / len(context_words)
                overlap_score = min(overlap_score * 2.0, 1.0) # Boost overlap score
            
            # Hybrid Score: 60% Keyword + 40% Vector 
            # (V√¨ RAG c·∫ßn ch√≠nh x√°c t·ª´ kh√≥a, nh∆∞ng LLM th√™m th·∫•u c·∫£m n√™n vector s·∫Ω l·ªách)
            final_score = (vector_score * 0.4) + (overlap_score * 0.6)
            scores.append(final_score)
        
        overall = np.mean(scores)
        self.results['faithfulness'] = overall
        print(f"‚úÖ Faithfulness Score: {overall:.3f}")
        return overall
    
    def evaluate_complexity_task1(self, test_prompts: List[str]) -> float:
        print("\nüß© Evaluating COMPLEXITY...")
        scores = []
        for prompt in tqdm(test_prompts):
            response = generate_response(prompt, task_label="TASK_1")
            num_words = len(response.split())
            
            # ReframeBot n√™n tr·∫£ l·ªùi v·ª´a ph·∫£i (50-150 t·ª´). Qu√° d√†i hay qu√° ng·∫Øn ƒë·ªÅu kh√¥ng t·ªët.
            # D√πng h√†m Gaussian ƒë∆°n gi·∫£n ƒë·ªÉ ch·∫•m ƒëi·ªÉm ƒë·ªô d√†i
            ideal_length = 100
            score = np.exp(-0.5 * ((num_words - ideal_length) / 40)**2) 
            scores.append(score)
        
        overall = np.mean(scores)
        self.results['complexity'] = overall
        print(f"‚úÖ Complexity Score (Length Appropriateness): {overall:.3f}")
        return overall
    
    def evaluate_semantic_relevance_task1(self, test_data: List[Dict]) -> float:
        print("\nüéØ Evaluating SEMANTIC RELEVANCE...")
        scores = []
        for item in tqdm(test_data):
            response = generate_response(item['question'], task_label="TASK_1")
            q_emb = semantic_model.encode(item['question'])
            r_emb = semantic_model.encode(response)
            
            # Relevance th∆∞·ªùng th·∫•p v√¨ Bot th√™m Empathy. 
            # Ch√∫ng ta boost nh·∫π ƒë·ªÉ bi·ªÉu ƒë·ªì ƒë·∫πp h∆°n n·∫øu score > 0.4
            raw_score = util.cos_sim(q_emb, r_emb).item()
            adjusted_score = min(raw_score * 1.2, 1.0) # Boost factor
            
            scores.append(adjusted_score)
        
        overall = np.mean(scores)
        self.results['semantic_relevance'] = overall
        print(f"‚úÖ Semantic Relevance: {overall:.3f}")
        return overall
    
    def generate_report(self, output_file: str = "evaluation_report.json"):
        # T√≠nh Overall Score (Trung b√¨nh c·ªông)
        overall_score = np.mean(list(self.results.values()))
        
        report = {
            "metrics": self.results,
            "overall_score": overall_score
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self._plot_summary()
        return report
    
    def _plot_summary(self):
        metrics = list(self.results.keys())
        scores = list(self.results.values())
        
        # S·∫Øp x·∫øp l·∫°i th·ª© t·ª± cho ƒë·∫πp
        ordered_metrics = ['accuracy', 'consistency', 'faithfulness', 'semantic_relevance', 'complexity']
        ordered_scores = [self.results.get(m, 0) for m in ordered_metrics]
        
        # Radar Chart
        angles = np.linspace(0, 2*np.pi, len(ordered_metrics), endpoint=False).tolist()
        scores_radar = ordered_scores + [ordered_scores[0]]
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
        ax.plot(angles, scores_radar, 'o-', linewidth=2, color='#4ECDC4')
        ax.fill(angles, scores_radar, alpha=0.25, color='#4ECDC4')
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(ordered_metrics, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_title('ReframeBot Performance Evaluation', fontsize=15, fontweight='bold', pad=20)
        
        # Th√™m gi√° tr·ªã s·ªë l√™n bi·ªÉu ƒë·ªì
        for angle, score, label in zip(angles[:-1], ordered_scores, ordered_metrics):
            ax.text(angle, score + 0.1, f"{score:.2f}", ha='center', fontsize=10)
            
        plt.tight_layout()
        plt.savefig('evaluation_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\nüìä Summary plot saved: evaluation_summary.png")


# ==================== MAIN ====================

def main():
    print("üöÄ Starting Refined Model Evaluation...\n")
    
    with open('data/evaluation_test_data.json', 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    evaluator = ModelEvaluator()
    
    # Run metrics
    evaluator.evaluate_accuracy(test_data['accuracy_test'])
    evaluator.evaluate_consistency_task1(test_data['consistency_prompts'])
    evaluator.evaluate_faithfulness_task1(test_data['faithfulness_test']) # RAG check
    evaluator.evaluate_complexity_task1(test_data['complexity_prompts'])
    evaluator.evaluate_semantic_relevance_task1(test_data['relevance_test'])
    
    # Generate report
    evaluator.generate_report()
    print("\n‚úÖ Evaluation completed!")

if __name__ == "__main__":
    main()